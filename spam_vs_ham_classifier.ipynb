{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Martin A posted:\n",
      "Tassos Papadopoulos, the Greek sculptor behind the plan, judged that the\n",
      " limestone of Mount Kerdylio, 70 miles east of Salonika and not far from the\n",
      " Mount Athos monastic community, was ideal for the patriotic sculpture. \n",
      " \n",
      " As well as Alexander's granite features, 240 ft high and 170 ft wide, a\n",
      " museum, a restored amphitheatre and car park for admiring crowds are\n",
      "planned\n",
      "---------------------\n",
      "So is this mountain limestone or granite?\n",
      "If it's limestone, it'll weather pretty fast.\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "4 DVDs Free +s&p Join Now\n",
      "http://us.click.yahoo.com/pt6YBB/NXiEAA/mG3HAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "print(ham_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help wanted.  We are a 14 year old fortune 500 company, that is\n",
      "growing at a tremendous rate.  We are looking for individuals who\n",
      "want to work from home.\n",
      "\n",
      "This is an opportunity to make an excellent income.  No experience\n",
      "is required.  We will train you.\n",
      "\n",
      "So if you are looking to be employed from home with a career that has\n",
      "vast opportunities, then go:\n",
      "\n",
      "http://www.basetel.com/wealthnow\n",
      "\n",
      "We are looking for energetic and self motivated people.  If that is you\n",
      "than click on the link and fill out the form, and one of our\n",
      "employement specialist will contact you.\n",
      "\n",
      "To be removed from our link simple go to:\n",
      "\n",
      "http://www.basetel.com/remove.html\n",
      "\n",
      "\n",
      "4139vOLW7-758DoDY1425FRhM1-764SMFc8513fCsLl40\n"
     ]
    }
   ],
   "source": [
    "print(spam_emails[6].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails).most_common()\n",
    "\n",
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(ham_emails + spam_emails)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "\n",
      "<head>\n",
      "<meta http-equiv=\"Content-Language\" content=\"en-us\">\n",
      "<meta name=\"GENERATOR\" content=\"Microsoft FrontPage 5.0\">\n",
      "<meta name=\"ProgId\" content=\"FrontPage.Editor.Document\">\n",
      "<meta http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1252\">\n",
      "<title>Norton AD</title>\n",
      "</head>\n",
      "\n",
      "<body>\n",
      "\n",
      "<table border=\"3\" cellspacing=\"3\" width=\"469\" cellpadding=\"3\" bgcolor=\"#000080\"  bordercolor=\"#FFFFFF\" style=\"border-collapse: collapse\" bordercolordark=\"#FFFFFF\">\n",
      "  <tr>\n",
      "    <td width=\"447\">&nbsp;\n",
      "      <table border=\"1\" width=\"100%\" bgcolor=\"#FFFFFF\" bordercolor=\"#000080\">\n",
      "        <tr>\n",
      "          <td width=\"100%\" align=\"center\"><font face=\"Impact\" size=\"4\" color=\"#CC0000\">ATTENTION:\n",
      "            This is a MUST for <u>ALL</u> Computer Users!!!</font></td>\n",
      "        </tr>\n",
      "      </table>\n",
      "      <p align=\"center\"><font size=\"3\" face=\"Verdana\"><b>&nbsp;<font color=\"#FFFFFF\">*NEW\n",
      "      - Special Package Deal!*</font></b></font></p>\n",
      "      <table border=\"8\" width=\"100%\" height=\"86\" bgcolor=\"#FFFFFF\" bo ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     \n",
      "          ATTENTION:\n",
      "            This is a MUST for ALL Computer Users!!!\n",
      "       *NEW\n",
      "      - Special Package Deal!*\n",
      "      <table border=\"8\" width=\"100%\" height=\"86\" bgcolor=\"#FFFFFF\" bo ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content().strip()[:1000]), \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "url_extractor = None\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'http': 7, 'com': 5, 'www': 4, 'yahoo': 4, 'to': 4, 'you': 3, 'in': 3, 'thi': 3, 'of': 3, 'net': 3, 'group': 3, 'tom': 2, 'cliktrik': 2, 'peopl': 2, 'famili': 2, 'me': 2, 'number': 2, 'jpg': 2, 'one': 2, 's': 2, 'i': 2, 'the': 2, 'my': 2, 'cafe': 2, 'forteana': 2, 'is': 2, 'frogston': 2, 'html': 2, 'unsubscrib': 2, 'r': 1, 'which': 1, 'm': 1, 'actual': 1, 'take': 1, 'photo': 1, 'both': 1, 'figur': 1, 'are': 1, 'fact': 1, 'waxwork': 1, 'wa': 1, 'mme': 1, 'tussaud': 1, 'all': 1, 'place': 1, 'sydney': 1, 'australia': 1, 't': 1, 'damn': 1, 'it': 1, 'had': 1, 'kid': 1, 'believ': 1, 'knew': 1, 'albert': 1, 'einstein': 1, 'well': 1, 'until': 1, 'smart': 1, 'ask': 1, 'just': 1, 'how': 1, 'old': 1, 'were': 1, 'now': 1, 'fel': 1, 'new': 1, 'back': 1, 'cafeforteana': 1, 'weird': 1, 'page': 1, 'athenet': 1, 'felinda': 1, 'weirdpag': 1, 'non': 1, 'text': 1, 'portion': 1, 'messag': 1, 'have': 1, 'been': 1, 'remov': 1, 'sponsor': 1, 'plan': 1, 'sell': 1, 'a': 1, 'home': 1, 'us': 1, 'click': 1, 'jnumbersnna': 1, 'y': 1, 'leaa': 1, 'mvfiaa': 1, 'numbergsolb': 1, 'tm': 1, 'from': 1, 'send': 1, 'an': 1, 'email': 1, 'egroup': 1, 'your': 1, 'use': 1, 'subject': 1, 'doc': 1, 'info': 1, 'term': 1}),\n",
       "       Counter({'number': 35, 'i': 17, 'exmh': 14, 'the': 14, 'in': 11, 'it': 11, 'window': 11, 'unseen': 11, 'to': 9, 'and': 8, 'pgp': 5, 'have': 5, 't': 5, 'name': 5, 'so': 5, 'that': 5, 'messag': 4, 'as': 4, 'is': 4, 'my': 4, 'border': 4, 'shade': 4, 'new': 4, 'class': 4, 'unseenwin': 4, 'com': 3, 'you': 3, 'with': 3, 'if': 3, 'don': 3, 'thi': 3, 'just': 3, 'folder': 3, 'use': 3, 'like': 3, 'all': 3, 'desktop': 3, 'enlighten': 3, 'then': 3, 'for': 3, 'a': 3, 'wa': 3, 'xy': 3, 'user': 3, 'begin': 2, 'manag': 2, 'see': 2, 'can': 2, 'put': 2, 'back': 2, 'of': 2, 'problem': 2, 'though': 2, 'how': 2, 'about': 2, 'show': 2, 'count': 2, 'cach': 2, 'than': 2, 'unless': 2, 're': 2, 'virtual': 2, 'what': 2, 'version': 2, 'know': 2, 'went': 2, 'up': 2, 'be': 2, 'englighten': 2, 'file': 2, 'found': 2, 'style': 2, 'numberxnumb': 2, 'screen': 2, 'offscreen': 2, 'pager_left_blu': 2, 'restart': 2, 'end': 2, 'signatur': 2, 'http': 2, 'redhat': 2, 'sign': 1, 'hash': 1, 'shanumb': 1, 'articl': 1, 'enumbervnumbermw': 1, 'pmenag': 1, 'dt': 1, 'ensim': 1, 'write': 1, 'll': 1, 'tri': 1, 'anoth': 1, 'get': 1, 'insid': 1, 'sane': 1, 'boundari': 1, 'ani': 1, 'other': 1, 'exmhunseen': 1, 'enabl': 1, 'option': 1, 'display': 1, 'next': 1, 'each': 1, 'greater': 1, 'realli': 1, 'need': 1, 'more': 1, 'sequenc': 1, 'paul': 1, 'hal': 1, 'note': 1, 'visibl': 1, 'even': 1, 'main': 1, 'minim': 1, 'howev': 1, 'look': 1, 'through': 1, 'prefer': 1, 'menu': 1, 'didn': 1, 'anyth': 1, 'resembl': 1, 'where': 1, 'm': 1, 'run': 1, 'vnumber': 1, 'creaki': 1, 'into': 1, 'twm': 1, 'no': 1, 'came': 1, 'fine': 1, 'go': 1, 'saw': 1, 'same': 1, 'behavior': 1, 'befor': 1, 'knew': 1, 'must': 1, 'someth': 1, 'dig': 1, 'e_sess': 1, 'xxxxxx': 1, 'snapshot': 1, 'set': 1, 'e': 1, 'rememb': 1, 'posit': 1, 'state': 1, 'etc': 1, 'kept': 1, 'at': 1, 'had': 1, 'boot': 1, 'tini': 1, 'well': 1, 'search': 1, 'section': 1, 'wh': 1, 'layer': 1, 'sticki': 1, 'skiptask': 1, 'skipwinlist': 1, 'skipfocu': 1, 'chang': 1, 'would': 1, 'reappear': 1, 'viewabl': 1, 'space': 1, 'ctrl': 1, 'alt': 1, 'bingo': 1, 'there': 1, 'quickli': 1, 'told': 1, 'forget': 1, 'everyth': 1, 'except': 1, 'now': 1, 'sveldt': 1, 'littl': 1, 'entri': 1, 'complet': 1, 'an': 1, 'not': 1, 'wonder': 1, 'got': 1, 'mix': 1, 'first': 1, 'place': 1, 'certainli': 1, 'move': 1, 'off': 1, 'part': 1, 'way': 1, 'over': 1, 'snap': 1, 'when': 1, 'hmm': 1, 'weird': 1, 'anyway': 1, 'thank': 1, 'suggest': 1, 'folk': 1, 'hope': 1, 'will': 1, 'someon': 1, 'scout': 1, 'archiv': 1, 'futur': 1, 'comment': 1, 'check': 1, 'www': 1, 'pgpi': 1, 'org': 1, 'iqa': 1, 'awubpzayrijkhjbjytpqeqiyzgcg': 1, 'mmmlknmpnumbercxa': 1, 'hnumberbnumberxnumberkrjxslsaoonumbern': 1, 'numberdmnumberhpjnumberrnugpjuuqitbyecnumb': 1, 'olnumberh': 1, '_______________________________________________': 1, 'mail': 1, 'list': 1, 'listman': 1, 'mailman': 1, 'listinfo': 1}),\n",
       "       Counter({'the': 115, 'and': 107, 'of': 94, 'to': 67, 'in': 64, 'number': 61, 'a': 60, 'xml': 56, 's': 51, 'technolog': 50, 'datapow': 47, 'network': 40, 'is': 32, 'for': 29, 'compani': 24, 'wa': 22, 'capit': 22, 'process': 21, 'at': 21, 'on': 20, 'that': 20, 'an': 19, 'with': 18, 'servic': 18, 'he': 18, 'as': 17, 'engin': 17, 'kelli': 16, 'ha': 16, 'enterpris': 16, 'develop': 16, 'by': 15, 'from': 15, 'devic': 15, 'ventur': 15, 'applic': 14, 'industri': 14, 'manag': 14, 'venrock': 14, 'it': 13, 'secur': 13, 'market': 13, 'gener': 13, 'data': 12, 'softwar': 12, 'provid': 12, 'acceler': 11, 'said': 11, 'be': 10, 'first': 10, 'base': 10, 'web': 10, 'mobiu': 10, 'seed': 10, 'solut': 9, 'product': 9, 'thi': 9, 'are': 9, 'kuznetsov': 9, 'associ': 9, 'internet': 9, 't': 8, 'busi': 8, 'awar': 8, 'over': 8, 'earli': 8, 'have': 8, 'their': 8, 'invest': 8, 'lead': 8, 'partner': 8, 'commun': 8, 'presid': 8, 'perform': 7, 'famili': 7, 'xgnumber': 7, 'steve': 7, 'will': 7, 'other': 7, 'switch': 7, 'comput': 7, 'inc': 7, 'entrepreneur': 7, 'posit': 7, 'taylor': 7, 'firewal': 6, 'server': 6, 'infrastructur': 6, 'also': 6, 'world': 6, 'found': 6, 'speed': 6, 'next': 6, 'such': 6, 'high': 6, 'been': 6, 'address': 6, 'protocol': 6, 'includ': 6, 'b': 6, 'patent': 6, 'founder': 6, 'inform': 6, 'where': 6, 'led': 6, 'vice': 6, 'i': 5, 'experi': 5, 'deliv': 5, 'xanumb': 5, 'today': 5, 'traffic': 5, 'more': 5, 'creat': 5, 'enabl': 5, 'use': 5, 'co': 5, 'now': 5, 'one': 5, 'team': 5, 'requir': 5, 'system': 5, 'fund': 5, 'pioneer': 5, 'director': 5, 'about': 5, 'optim': 5, 'stage': 5, 'year': 5, 'com': 5, 'held': 5, 'prior': 5, 'willi': 5, 'univers': 5, 'tao': 5, 'effort': 4, 'm': 4, 'design': 4, 'hardwar': 4, 'new': 4, 'cambridg': 4, 'core': 4, 'addit': 4, 'inspect': 4, 'packet': 4, 'most': 4, 'execut': 4, 'hi': 4, 'knowledg': 4, 'can': 4, 'start': 4, 'custom': 4, 'sinc': 4, 'eugen': 4, 'three': 4, 'power': 4, 'akamai': 4, 'million': 4, 'bill': 4, 'pend': 4, 'support': 4, 'firm': 4, 'scienc': 4, 'well': 4, 'corpor': 4, 'area': 4, 'which': 4, 'kieran': 4, 'work': 4, 'java': 4, 'hoover': 4, 'rose': 4, 'not': 3, 'state': 3, 'help': 3, 'but': 3, 'proxi': 3, 'announc': 3, 'what': 3, 'go': 3, 'best': 3, 'specif': 3, 'accord': 3, 'ceo': 3, 'adopt': 3, 'up': 3, 'time': 3, 'focu': 3, 'initi': 3, 'load': 3, 'balanc': 3, 'look': 3, 'launch': 3, 'long': 3, 'befor': 3, 'grew': 3, 'built': 3, 'third': 3, 'return': 3, 'sever': 3, 'argon': 3, 'cascad': 3, 'castl': 3, 'wellfleet': 3, 'join': 3, 'sale': 3, 'intellig': 3, 'cto': 3, 'critic': 3, 'grow': 3, 'span': 3, 'opportun': 3, 'contact': 3, 'visit': 3, 'www': 3, 'u': 3, 'senior': 3, 'profession': 3, 'forc': 3, 'consult': 3, 'unispher': 3, 'electron': 3, 'hold': 3, 'electr': 3, 'bay': 3, 'atm': 3, 'rout': 3, 'kassabgi': 3, 'attack': 2, 'origin': 2, 'messag': 2, 'rohit': 2, 'khare': 2, 'august': 2, 'subject': 2, 'no': 2, 'analysi': 2, 'yet': 2, 'make': 2, 'all': 2, 'out': 2, 'greater': 2, 'expect': 2, 'mass': 2, 'startup': 2, 'call': 2, 'doe': 2, 'futur': 2, 'prolifer': 2, 'between': 2, 'two': 2, 'our': 2, 'vision': 2, 'build': 2, 'explain': 2, 'convert': 2, 'file': 2, 'size': 2, 'veri': 2, 'believ': 2, 'larg': 2, 'onli': 2, 'through': 2, 'approach': 2, 'itself': 2, 'tri': 2, 'would': 2, 'month': 2, 'claim': 2, 'content': 2, 'byte': 2, 'while': 2, 'against': 2, 'type': 2, 'languag': 2, 'pass': 2, 'deploy': 2, 'mode': 2, 'identifi': 2, 'give': 2, 'control': 2, 'emerg': 2, 'sarvega': 2, 'ill': 2, 'intel': 2, 'date': 2, 'need': 2, 'visionari': 2, 'who': 2, 'effect': 2, 'uniqu': 2, 'licens': 2, 'own': 2, 'bar': 2, 'fastest': 2, 'wire': 2, 'immedi': 2, 'bring': 2, 'decad': 2, 'leader': 2, 'success': 2, 'sycamor': 2, 'back': 2, 'financ': 2, 'investor': 2, 'michael': 2, 'tyrrel': 2, 'burnham': 2, 'jeff': 2, 'fagnan': 2, 'board': 2, 'capabl': 2, 'rate': 2, 'facilit': 2, 'wirespe': 2, 'without': 2, 'singl': 2, 'real': 2, 'focus': 2, 'we': 2, 'problem': 2, 'key': 2, 'rockefel': 2, 'continu': 2, 'tradit': 2, 'establish': 2, 'record': 2, 'relat': 2, 'appl': 2, 'ma': 2, 'pleas': 2, 'formerli': 2, 'softbank': 2, 'billion': 2, 'privat': 2, 'unparallel': 2, 'primarili': 2, 'expertis': 2, 'portfolio': 2, 'digit': 2, 'region': 2, 'spearhead': 2, 'schwartz': 2, 'dure': 2, 'fortun': 2, 'acquir': 2, 'siemen': 2, 'ag': 2, 'subsequ': 2, 'purchas': 2, 'junip': 2, 'contributor': 2, 'annual': 2, 'largest': 2, 'advanc': 2, 'accomplish': 2, 'router': 2, 'nortel': 2, 'group': 2, 'current': 2, 'research': 2, 'task': 2, 'wan': 2, 'previous': 2, 'receiv': 2, 'analyst': 2, 'technic': 2, 'contribut': 2, 'wide': 2, 'public': 2, 'journal': 2, 'mark': 2, 'acuit': 2, 'edg': 2, 'georg': 2, 'known': 2, 'author': 2, 'marshal': 2, 'if': 1, 'wouldn': 1, 'seem': 1, 'worth': 1, 'although': 1, 'guess': 1, 'might': 1, 'ddo': 1, 'sound': 1, 'snake': 1, 'oilish': 1, 'me': 1, 'bias': 1, 'lot': 1, 'limit': 1, 'thereof': 1, 'ken': 1, 'mailto': 1, 'alumni': 1, 'caltech': 1, 'edu': 1, 'sent': 1, 'tuesday': 1, 'pm': 1, 'fork': 1, 'spamassassin': 1, 'taint': 1, 'org': 1, 'silicon': 1, 'don': 1, 'know': 1, 'here': 1, 'raw': 1, 'bit': 1, 'perus': 1, 'check': 1, 'realli': 1, 'scott': 1, 'tyler': 1, 'shafer': 1, 'am': 1, 'pt': 1, 'monday': 1, 'unveil': 1, 'unlik': 1, 'compet': 1, 'achiev': 1, 'offici': 1, 'dub': 1, 'proprietari': 1, 'pars': 1, 'conceiv': 1, 'meet': 1, 'steadi': 1, 'anticip': 1, 'mean': 1, 'share': 1, 'into': 1, 'increas': 1, 'tax': 1, 'inlin': 1, 'altern': 1, 'paramount': 1, 'import': 1, 'http': 1, 'soap': 1, 'straight': 1, 'blind': 1, 'especi': 1, 'exist': 1, 'offload': 1, 'homegrown': 1, 'done': 1, 'inadequ': 1, 'regard': 1, 'after': 1, 'path': 1, 'turn': 1, 'machin': 1, 'code': 1, 'ad': 1, 'few': 1, 'flood': 1, 'do': 1, 'mostli': 1, 'fals': 1, 'ssl': 1, 'socket': 1, 'layer': 1, 'deeper': 1, 'thu': 1, 'numberu': 1, 'test': 1, 'collect': 1, 'xsl': 1, 'learn': 1, 'flavor': 1, 'markup': 1, 'they': 1, 'behind': 1, 'those': 1, 'processor': 1, 'method': 1, 'administr': 1, 'granular': 1, 'chase': 1, 'burr': 1, 'ridg': 1, 'introduc': 1, 'xpe': 1, 'may': 1, 'earlier': 1, 'tarari': 1, 'spin': 1, 'off': 1, 'avail': 1, 'price': 1, 'say': 1, 'field': 1, 'trail': 1, 'foresaw': 1, 'advers': 1, 'interest': 1, 'assembl': 1, 'class': 1, 'interpret': 1, 'dgxt': 1, 'still': 1, 'mani': 1, 'leverag': 1, 'detail': 1, 'gain': 1, 'rais': 1, 'purpos': 1, 'produc': 1, 'none': 1, 'just': 1, 'version': 1, 'heritag': 1, 'complement': 1, 'draw': 1, 'veteran': 1, 'juli': 1, 'seri': 1, 'round': 1, 'breakthrough': 1, 'invent': 1, 'transform': 1, 'transact': 1, 'embrac': 1, 'centric': 1, 'rapid': 1, 'inter': 1, 'intra': 1, 'prepar': 1, 'abil': 1, 'project': 1, 'sacrif': 1, 'ounc': 1, 'break': 1, 'bottleneck': 1, 'extraordinari': 1, 'platform': 1, 'enhanc': 1, 'protect': 1, 'denial': 1, 'connect': 1, 'e': 1, 'incompat': 1, 'stream': 1, 'end': 1, 'statist': 1, 'report': 1, 'post': 1, 'bubbl': 1, 'economi': 1, 'decis': 1, 'laser': 1, 'scrutini': 1, 'pain': 1, 'point': 1, 'turbo': 1, 'charg': 1, 'free': 1, 'encount': 1, 'pitfal': 1, 'hard': 1, 'rapidli': 1, 'surround': 1, 'quit': 1, 'simpli': 1, 'compel': 1, 'seen': 1, 'nice': 1, 'must': 1, 'seriou': 1, 'effici': 1, 'reli': 1, 'mission': 1, 'arm': 1, 'seven': 1, 'lauranc': 1, 'than': 1, 'track': 1, 'promis': 1, 'experienc': 1, 'unit': 1, 'maintain': 1, 'collabor': 1, 'talent': 1, 'endur': 1, 'goal': 1, 'term': 1, 'valu': 1, 'assist': 1, 'form': 1, 'consist': 1, 'life': 1, 'reservoir': 1, 'proven': 1, 'catalyst': 1, 'growth': 1, 'organ': 1, 'checkpoint': 1, 'usinternetwork': 1, 'calip': 1, 'illumina': 1, 'niku': 1, 'doubleclick': 1, 'media': 1, 'metrix': 1, 'numbercom': 1, 'offic': 1, 'york': 1, 'citi': 1, 'menlo': 1, 'park': 1, 'ca': 1, 'respond': 1, 'ani': 1, 'local': 1, 'equiti': 1, 'former': 1, 'major': 1, 'bank': 1, 'special': 1, 'healthcar': 1, 'informat': 1, 'consum': 1, 'small': 1, 'compon': 1, 'combin': 1, 'broad': 1, 'financi': 1, 'asset': 1, 'powerhous': 1, 'site': 1, 'mobiusvc': 1, 'affili': 1, 'locat': 1, 'northeastern': 1, 'southeastern': 1, 'provinc': 1, 'ontario': 1, 'canada': 1, 'israel': 1, 'concentr': 1, 'companydna': 1, 'seedcp': 1, 'ensur': 1, 'john': 1, 'moran': 1, 'heather': 1, 'chichakli': 1, 'pr': 1, 'chairman': 1, 'twenti': 1, 'global': 1, 'involv': 1, 'resid': 1, 'function': 1, 'revenu': 1, 'tenur': 1, 'equip': 1, 'node': 1, 'countri': 1, 'bentley': 1, 'colleg': 1, 'issu': 1, 'late': 1, 'jit': 1, 'compil': 1, 'microsoft': 1, 'explor': 1, 'macintosh': 1, 'part': 1, 'clean': 1, 'room': 1, 'vm': 1, 'runtim': 1, 'some': 1, 'numer': 1, 'varieti': 1, 'memori': 1, 'integr': 1, 'mit': 1, 'both': 1, 'ip': 1, 'mbp': 1, 'forward': 1, 'forum': 1, 'pnni': 1, 'notabl': 1, 'architect': 1, 'multi': 1, 'consecut': 1, 'magazin': 1, 'member': 1, 'institut': 1, 'ieee': 1, 'irtf': 1, 'd': 1, 'c': 1, 'massachusett': 1, 'vast': 1, 'understand': 1, 'extens': 1, 'lan': 1, 'metro': 1, 'optic': 1, 'acquaint': 1, 'techniqu': 1, 'infolibria': 1, 'qualiti': 1, 'assur': 1, 'cach': 1, 'netedg': 1, 'proteon': 1, 'codex': 1, 'wang': 1, 'connecticut': 1, 'illinoi': 1, 'journalist': 1, 'nasdaq': 1, 'akam': 1, 'activ': 1, 'flagship': 1, 'edgesuit': 1, 'ipo': 1, 'telechoic': 1, 'editor': 1, 'mcgraw': 1, 'hill': 1, 'print': 1, 'pennsylvania': 1, 'school': 1, 'advisor': 1, 'bell': 1, 'laboratori': 1, 'synopt': 1, 'play': 1, 'role': 1, 'baset': 1, 'fddi': 1, 'ethernet': 1, 'bea': 1, 'mr': 1, 'level': 1, 'marketplac': 1, 'regular': 1, 'speak': 1, 'engag': 1, 'javaon': 1, 'column': 1, 'javapro': 1, 'vener': 1, 'expert': 1, 'smartobject': 1, 'book': 1, 'progress': 1, 'vnumber': 1, 'run': 1, 'dover': 1, 'beach': 1, 'ietf': 1, 'dozen': 1, 'individu': 1, 'oversaw': 1, 'standard': 1, 'text': 1, 'mail': 1, 'directori': 1, 'publish': 1, 'four': 1, 'implement': 1, 'pop': 1, 'smtp': 1, 'snmp': 1, 'osi': 1, 'x': 1, 'ftam': 1, 'phd': 1, 'california': 1, 'irvin': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.longlong'>'\n",
       "\twith 32 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 132,    3,    4,    2,    2,    1,    0,    2,    2,    3,    3],\n",
       "       [ 373,   11,    9,   35,   14,   11,    8,   17,    4,    3,    2],\n",
       "       [2447,   64,   67,   61,  115,   13,  107,    5,   32,    9,   94]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=24, max_iter=100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=1.000, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=1.000, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.2s remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.983, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.3s remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.967, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.983, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.5s remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=1.000, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.6s remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=1.000, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.7s remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.967, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.8s remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=1.000, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.9s remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.983, total=   0.1s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9883333333333333"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=10, verbose=1000)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed =preprocess_pipeline.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=24, max_iter=100000000)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "y_pred = log_clf.predict(X_test_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 97.18%\n",
      "Recall: 89.61%\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 97.18%\n",
      "Recall: 89.61%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=24, max_iter=100000000)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 97.18%\n",
      "Recall: 89.61%\n",
      "Precision of svc: 94.79%\n",
      "Recall of svc: 89.87%\n",
      "Precision of ran: 98.40%\n",
      "Recall of ran: 80.00%\n",
      "Precision of xgb: 98.25%\n",
      "Recall of xgb: 87.27%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "svc_clf = SVC(kernel=\"linear\", random_state=24, max_iter=100000000)\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=24, max_iter=100000000)\n",
    "ran_clf = RandomForestClassifier(random_state=24)\n",
    "xgb_clf= XGBClassifier(random_state=24)\n",
    "\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "ran_clf.fit(X_train_transformed, y_train)\n",
    "svc_clf.fit(X_train_transformed, y_train)\n",
    "xgb_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred_xgb = xgb_clf.predict(X_test_transformed)\n",
    "y_pred_ran = ran_clf.predict(X_test_transformed)\n",
    "y_pred_svc = svc_clf.predict(X_test_transformed)\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n",
    "print(\"Precision of svc: {:.2f}%\".format(100 * precision_score(y_test, y_pred_svc)))\n",
    "print(\"Recall of svc: {:.2f}%\".format(100 * recall_score(y_test, y_pred_svc)))\n",
    "print(\"Precision of ran: {:.2f}%\".format(100 * precision_score(y_test, y_pred_ran)))\n",
    "print(\"Recall of ran: {:.2f}%\".format(100 * recall_score(y_test, y_pred_ran)))\n",
    "print(\"Precision of xgb: {:.2f}%\".format(100 * precision_score(y_test, y_pred_xgb)))\n",
    "print(\"Recall of xgb: {:.2f}%\".format(100 * recall_score(y_test, y_pred_xgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
    "def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
    "        tar_bz2_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "def load_email(is_spam, filename, spam_path=SPAM_PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory, filename), \"rb\") as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 218),\n",
       " ('text/html', 183),\n",
       " ('multipart(text/plain, text/html)', 45),\n",
       " ('multipart(text/html)', 20),\n",
       " ('multipart(text/plain)', 19),\n",
       " ('multipart(multipart(text/html))', 5),\n",
       " ('multipart(text/plain, image/jpeg)', 3),\n",
       " ('multipart(text/html, application/octet-stream)', 2),\n",
       " ('multipart(text/plain, application/octet-stream)', 1),\n",
       " ('multipart(text/html, text/plain)', 1),\n",
       " ('multipart(multipart(text/html), application/octet-stream, image/jpeg)', 1),\n",
       " ('multipart(multipart(text/plain, text/html), image/gif)', 1),\n",
       " ('multipart/alternative', 1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures\n",
    "\n",
    "\n",
    "structures_counter(ham_emails).most_common()\n",
    "\n",
    "\n",
    "structures_counter(spam_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(ham_emails + spam_emails)\n",
    "y = np.array([0] * len(ham_emails) + [1] * len(spam_emails))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('<head.*?>.*?</head>', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML><HEAD><TITLE></TITLE><META http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1252\"><STYLE>A:link {TEX-DECORATION: none}A:active {TEXT-DECORATION: none}A:visited {TEXT-DECORATION: none}A:hover {COLOR: #0033ff; TEXT-DECORATION: underline}</STYLE><META content=\"MSHTML 6.00.2713.1100\" name=\"GENERATOR\"></HEAD>\n",
      "<BODY text=\"#000000\" vLink=\"#0033ff\" link=\"#0033ff\" bgColor=\"#CCCC99\"><TABLE borderColor=\"#660000\" cellSpacing=\"0\" cellPadding=\"0\" border=\"0\" width=\"100%\"><TR><TD bgColor=\"#CCCC99\" valign=\"top\" colspan=\"2\" height=\"27\">\n",
      "<font size=\"6\" face=\"Arial, Helvetica, sans-serif\" color=\"#660000\">\n",
      "<b>OTC</b></font></TD></TR><TR><TD height=\"2\" bgcolor=\"#6a694f\">\n",
      "<font size=\"5\" face=\"Times New Roman, Times, serif\" color=\"#FFFFFF\">\n",
      "<b>&nbsp;Newsletter</b></font></TD><TD height=\"2\" bgcolor=\"#6a694f\"><div align=\"right\"><font color=\"#FFFFFF\">\n",
      "<b>Discover Tomorrow's Winners&nbsp;</b></font></div></TD></TR><TR><TD height=\"25\" colspan=\"2\" bgcolor=\"#CCCC99\"><table width=\"100%\" border=\"0\"  ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "url_extractor = None\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'r': 1}),\n",
       "       Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'christian': 3, 'to': 3, 'by': 3, 'jefferson': 2, 'i': 2, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'ha': 2, 'half': 2, 'rogueri': 2, 'teach': 2, 'jesu': 2, 'some': 1, 'interest': 1, 'quot': 1, 'http': 1, 'www': 1, 'postfun': 1, 'com': 1, 'pfp': 1, 'worboi': 1, 'html': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'what': 1, 'effect': 1, 'thi': 1, 'coercion': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'over': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'e': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'again': 1, 'becom': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1}),\n",
       "       Counter({'number': 5, 'http': 4, 'yahoo': 4, 's': 3, 'group': 3, 'com': 3, 'to': 3, 'in': 2, 'forteana': 2, 'martin': 2, 'an': 2, 'and': 2, 'memri': 2, 'we': 2, 'is': 2, 'unsubscrib': 2, 'y': 1, 'adamson': 1, 'wrote': 1, 'for': 1, 'altern': 1, 'rather': 1, 'more': 1, 'factual': 1, 'base': 1, 'rundown': 1, 'on': 1, 'hamza': 1, 'career': 1, 'includ': 1, 'hi': 1, 'belief': 1, 'that': 1, 'all': 1, 'non': 1, 'muslim': 1, 'yemen': 1, 'should': 1, 'be': 1, 'murder': 1, 'outright': 1, 'org': 1, 'bin': 1, 'articl': 1, 'cgi': 1, 'page': 1, 'archiv': 1, 'area': 1, 'ia': 1, 'id': 1, 'ianumb': 1, 'know': 1, 'how': 1, 'unbias': 1, 'don': 1, 't': 1, 'www': 1, 'guardian': 1, 'co': 1, 'uk': 1, 'elsewher': 1, 'journalist': 1, 'stori': 1, 'html': 1, 'rob': 1, 'sponsor': 1, 'dvd': 1, 'free': 1, 'p': 1, 'join': 1, 'now': 1, 'us': 1, 'click': 1, 'ptnumberybb': 1, 'nxieaa': 1, 'mvfiaa': 1, 'numbergsolb': 1, 'tm': 1, 'from': 1, 'thi': 1, 'send': 1, 'email': 1, 'egroup': 1, 'your': 1, 'use': 1, 'of': 1, 'subject': 1, 'doc': 1, 'info': 1, 'term': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [112,  11,   9,   8,   3,   1,   0,   1,   3,   0,   1],\n",
       "       [ 92,   0,   1,   2,   3,   4,   5,   3,   1,   4,   2]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.most_common_ = most_common\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))\n",
    "\n",
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors\n",
    "\n",
    "X_few_vectors.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/usr/local/anaconda/envs/tensorflow2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.983, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.984, total=   0.1s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.993, total=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/envs/tensorflow2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n",
      "/usr/local/anaconda/envs/tensorflow2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9862500000000001"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 96.88%\n",
      "Recall: 97.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/envs/tensorflow2/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:939: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html.\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
